{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from arize.otel import register\n",
    "from dotenv import load_dotenv\n",
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "\n",
    "from src.python.generate_queries import generate_questions_from_files\n",
    "from src.python.RagRetriever import RAGRetriever\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "tracer_provider = register(\n",
    "    space_id = os.getenv(\"ARIZE_SPACE_ID\"),\n",
    "    api_key =  os.getenv(\"ARIZE_API_KEY\"),\n",
    "    project_name = \"rag-app-notebook\",\n",
    ")\n",
    "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "\n",
    "\n",
    "rag_retriever = RAGRetriever(\n",
    "    collection_name=\"chunk_size_500\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = \"./data/arxiv_papers\"\n",
    "questions = generate_questions_from_files(pdf_dir, max_files=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_query(rag_retriever, query: str, similarity_top_k: int = 20, rerank_top_n: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single query and record timing information.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = rag_retriever.similarity_search(query, similarity_top_k=similarity_top_k, rerank_top_n=rerank_top_n)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"time\": time_taken,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "def run_concurrent_queries(rag_retriever, queries: List[str], max_workers: int = 4, \n",
    "                         similarity_top_k: int = 20, rerank_top_n: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Run queries concurrently with a progress bar and return timing results.\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_query, \n",
    "                rag_retriever, \n",
    "                query, \n",
    "                similarity_top_k, \n",
    "                rerank_top_n\n",
    "            ) for query in queries\n",
    "        ]\n",
    "        \n",
    "        for future in tqdm(\n",
    "            concurrent.futures.as_completed(futures),\n",
    "            total=len(futures),\n",
    "            desc=\"Processing queries\",\n",
    "            unit=\"query\"\n",
    "        ):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                # Just log the error without interrupting the progress bar\n",
    "                all_results.append({\"time\": None, \"error\": str(e)})\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def calculate_percentiles(times: List[float]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate various percentiles of the timing data.\"\"\"\n",
    "    percentiles = {\n",
    "        \"p50\": np.percentile(times, 50),\n",
    "        \"p75\": np.percentile(times, 75),\n",
    "        \"p90\": np.percentile(times, 90),\n",
    "        \"p95\": np.percentile(times, 95),\n",
    "        \"p99\": np.percentile(times, 99),\n",
    "    }\n",
    "    return percentiles\n",
    "\n",
    "max_workers = 4\n",
    "\n",
    "results = run_concurrent_queries(\n",
    "    rag_retriever, \n",
    "    questions, \n",
    "    max_workers=max_workers,\n",
    "    similarity_top_k=20, \n",
    "    rerank_top_n=3\n",
    ")\n",
    "\n",
    "query_times = [result[\"time\"] for result in results if result[\"time\"] is not None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Latency Statistics:\n",
      "----------------------------------------\n",
      "Average: 0.5096s\n",
      "Minimum: 0.1914s\n",
      "Maximum: 6.8483s\n",
      "\n",
      "Percentile Latencies:\n",
      "p50 (median): 0.3454s\n",
      "p75: 0.4359s\n",
      "p90: 0.6828s\n",
      "p95: 1.1492s\n",
      "p99: 4.9238s\n",
      "\n",
      "Standard deviation: 0.7121s\n",
      "\n",
      "Total queries: 522\n",
      "Failed queries: 0\n",
      "\n",
      "Total sequential processing time: 266.0272s\n",
      "Wall clock time with concurrency: 6.8483s\n",
      "Speedup from concurrency: 38.85x\n"
     ]
    }
   ],
   "source": [
    "if query_times:\n",
    "    avg_time = statistics.mean(query_times)\n",
    "    min_time = min(query_times)\n",
    "    max_time = max(query_times)\n",
    "    \n",
    "    percentiles = calculate_percentiles(query_times)\n",
    "    \n",
    "    print(\"\\nQuery Latency Statistics:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Average: {avg_time:.4f}s\")\n",
    "    print(f\"Minimum: {min_time:.4f}s\")\n",
    "    print(f\"Maximum: {max_time:.4f}s\")\n",
    "    \n",
    "    print(\"\\nPercentile Latencies:\")\n",
    "    print(f\"p50 (median): {percentiles['p50']:.4f}s\")\n",
    "    print(f\"p75: {percentiles['p75']:.4f}s\")\n",
    "    print(f\"p90: {percentiles['p90']:.4f}s\")\n",
    "    print(f\"p95: {percentiles['p95']:.4f}s\")\n",
    "    print(f\"p99: {percentiles['p99']:.4f}s\")\n",
    "    \n",
    "    if len(query_times) > 1:\n",
    "        stddev = statistics.stdev(query_times)\n",
    "        print(f\"\\nStandard deviation: {stddev:.4f}s\")\n",
    "    \n",
    "    print(f\"\\nTotal queries: {len(query_times)}\")\n",
    "    print(f\"Failed queries: {len(results) - len(query_times)}\")\n",
    "    \n",
    "    total_sequential_time = sum(query_times)\n",
    "    elapsed_wall_time = max(result[\"time\"] for result in results if result[\"time\"] is not None)\n",
    "    \n",
    "    print(f\"\\nTotal sequential processing time: {total_sequential_time:.4f}s\")\n",
    "    print(f\"Wall clock time with concurrency: {elapsed_wall_time:.4f}s\")\n",
    "    print(f\"Speedup from concurrency: {total_sequential_time / elapsed_wall_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian.ogrady/src/python3/pepsi/RAG/rag-with-reranking-streamlit/src/python/RagRetriever.py:204: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return self.base_retriever.get_relevant_documents(query)\n",
      "Processing queries: 100%|██████████| 1000/1000 [02:01<00:00,  8.23query/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Latency Statistics:\n",
      "----------------------------------------\n",
      "Average: 0.4795s\n",
      "Minimum: 0.1698s\n",
      "Maximum: 8.3467s\n",
      "\n",
      "Percentile Latencies:\n",
      "p50 (median): 0.3598s\n",
      "p75: 0.4461s\n",
      "p90: 0.6161s\n",
      "p95: 0.8421s\n",
      "p99: 3.6308s\n",
      "\n",
      "Standard deviation: 0.6205s\n",
      "\n",
      "Total queries: 1000\n",
      "Failed queries: 0\n",
      "\n",
      "Total sequential processing time: 479.5331s\n",
      "Wall clock time with concurrency: 8.3467s\n",
      "Speedup from concurrency: 57.45x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import statistics\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_query(rag_retriever, query: str, similarity_top_k: int = 20, rerank_top_n: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single query and record timing information.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = rag_retriever.get_relevant_documents(query)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"time\": time_taken,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "def run_concurrent_queries(rag_retriever, queries: List[str], max_workers: int = 4, \n",
    "                         similarity_top_k: int = 20, rerank_top_n: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Run queries concurrently with a progress bar and return timing results.\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_query, \n",
    "                rag_retriever, \n",
    "                query, \n",
    "                similarity_top_k, \n",
    "                rerank_top_n\n",
    "            ) for query in queries\n",
    "        ]\n",
    "        \n",
    "        for future in tqdm(\n",
    "            concurrent.futures.as_completed(futures),\n",
    "            total=len(futures),\n",
    "            desc=\"Processing queries\",\n",
    "            unit=\"query\"\n",
    "        ):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                # Just log the error without interrupting the progress bar\n",
    "                all_results.append({\"time\": None, \"error\": str(e)})\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def calculate_percentiles(times: List[float]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate various percentiles of the timing data.\"\"\"\n",
    "    percentiles = {\n",
    "        \"p50\": np.percentile(times, 50),\n",
    "        \"p75\": np.percentile(times, 75),\n",
    "        \"p90\": np.percentile(times, 90),\n",
    "        \"p95\": np.percentile(times, 95),\n",
    "        \"p99\": np.percentile(times, 99),\n",
    "    }\n",
    "    return percentiles\n",
    "\n",
    "max_workers = 4\n",
    "\n",
    "results = run_concurrent_queries(\n",
    "    rag_retriever, \n",
    "    questions, \n",
    "    max_workers=max_workers,\n",
    "    similarity_top_k=20, \n",
    "    rerank_top_n=3\n",
    ")\n",
    "\n",
    "query_times = [result[\"time\"] for result in results if result[\"time\"] is not None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_times:\n",
    "    avg_time = statistics.mean(query_times)\n",
    "    min_time = min(query_times)\n",
    "    max_time = max(query_times)\n",
    "    \n",
    "    percentiles = calculate_percentiles(query_times)\n",
    "    \n",
    "    print(\"\\nQuery Latency Statistics:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Average: {avg_time:.4f}s\")\n",
    "    print(f\"Minimum: {min_time:.4f}s\")\n",
    "    print(f\"Maximum: {max_time:.4f}s\")\n",
    "    \n",
    "    print(\"\\nPercentile Latencies:\")\n",
    "    print(f\"p50 (median): {percentiles['p50']:.4f}s\")\n",
    "    print(f\"p75: {percentiles['p75']:.4f}s\")\n",
    "    print(f\"p90: {percentiles['p90']:.4f}s\")\n",
    "    print(f\"p95: {percentiles['p95']:.4f}s\")\n",
    "    print(f\"p99: {percentiles['p99']:.4f}s\")\n",
    "    \n",
    "    if len(query_times) > 1:\n",
    "        stddev = statistics.stdev(query_times)\n",
    "        print(f\"\\nStandard deviation: {stddev:.4f}s\")\n",
    "    \n",
    "    print(f\"\\nTotal queries: {len(query_times)}\")\n",
    "    print(f\"Failed queries: {len(results) - len(query_times)}\")\n",
    "    \n",
    "    total_sequential_time = sum(query_times)\n",
    "    elapsed_wall_time = max(result[\"time\"] for result in results if result[\"time\"] is not None)\n",
    "    \n",
    "    print(f\"\\nTotal sequential processing time: {total_sequential_time:.4f}s\")\n",
    "    print(f\"Wall clock time with concurrency: {elapsed_wall_time:.4f}s\")\n",
    "    print(f\"Speedup from concurrency: {total_sequential_time / elapsed_wall_time:.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pepsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
